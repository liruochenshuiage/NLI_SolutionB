# COMP34812CW_Solution_B
## 1. Overview
This project presents a BiLSTM-based neural model (Category B) for the COMP34812 shared task on Natural Language Inference (NLI). The model combines CNN, Bidirectional LSTM, and an attention mechanism to capture local n-gram features, sequential dependencies, and key contextual information in sentence pairs.

To improve generalization and mitigate overfitting, we apply dropout regularization, batch normalization, and early stopping. Key hyperparameters such as embedding dimension, LSTM units, and dropout rate were tuned manually using the validation set.

The model was trained on the provided training data and evaluated on the development set (dev.csv). All predictions, the final trained model, and preprocessing components (e.g., tokenizer) are included in this submission.

## 2.Requirements
This project requires Python 3.10 or later and the following Python libraries:

- tensorflow
- keras
- numpy
- pandas
- matplotlib
- scikit-learn

## 3.Running the Code
### 3.1 Running in Kaggle
Both the training and demo notebooks for Solution B (BiLSTM) were implemented and executed entirely in the Kaggle environment.

- **Training Notebook**: [comp34812-cw-b](https://www.kaggle.com/code/ruochen666/solution-b)  
- **Demo Notebook**: [comp34812-cw-b-demo](https://www.kaggle.com/code/ruochen666/solutionb-demo)


All required input files (e.g., `train.csv`, `dev.csv`) are already located in the appropriate Kaggle directory paths.

To run the notebooks:

1. Click the **"Copy & Edit"** button in the top right corner of the notebook page to duplicate the notebook into your workspace.
2. Once opened, click the **"Run All"** button from the menu bar to execute all cells sequentially.

> If a **network error or kernel disconnection** occurs, click the **"Factory Reset Runtime"** button (top right), then click **"Run All"** again to restart from a clean environment.

There is **no need to install any additional dependencies manually**. All required packages are automatically installed at the beginning of each notebook.

## 4. Saved Model and Output Files
### 4.1 Saved Model
Saved Model and Output Files
4.1 Saved Model
Only the best-performing BiLSTM model is retained after training:

- **BiLSTM model**: https://drive.google.com/file/d/177nzy7mqlgnjNIy7yFzi1JcmTpph8fLe/view?usp=sharing
This is the final Keras model file, saved using model.save(...) after training.
It includes all layers, weights, and custom attention logic, and can be directly loaded using tensorflow.keras.models.load_model(...) with custom_objects.

In addition, we also saved the tokenizer used during training:
- **Tokenizer**: https://drive.google.com/file/d/1oSqBE31T8w7TCwYd_sMyITR7iqNNDZWR/view?usp=sharing
This pickle file contains the fitted Keras Tokenizer instance.
It is required for preparing test inputs during inference.

Both files (.h5 and .pkl) are loaded by the demo notebook during prediction.

### 4.2 Prediction Outputs
- `predictions.csv`
Generated by the demo notebook (solutionB-demo.ipynb) using the provided test.csv file as input



